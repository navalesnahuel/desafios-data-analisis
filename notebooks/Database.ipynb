{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a61b45f-385f-43ea-b9d8-0bb03d29ecde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "24/08/22 04:08:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .config(\"spark.jars\", \"\"\"\n",
    "            /usr/local/spark/jars/aws-java-sdk-bundle-1.12.262.jar,\n",
    "            /usr/local/spark/jars/antlr4-runtime-4.9.3.jar,\n",
    "            /usr/local/spark/jars/delta-contribs_2.12-3.1.0.jar,\n",
    "            /usr/local/spark/jars/delta-iceberg_2.12-3.1.0.jar,\n",
    "            /usr/local/spark/jars/delta-spark_2.12-3.1.0.jar,\n",
    "            /usr/local/spark/jars/delta-storage-3.1.0.jar,\n",
    "            /usr/local/spark/jars/hadoop-aws-3.3.4.jar\"\"\") \\\n",
    "    .appName(\"SparkJupyter\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "%load_ext sparksql_magic\n",
    "%config SparkSql.limit=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e880fac-4770-4f6e-af88-947a66097d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS BIdatabase\")\n",
    "spark.sql(\"USE BIdatabase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec18c92-4389-4fa4-8ffd-7c2495e876e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://data/new_retail_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796042b3-9e44-401e-a855-07b48ed24c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, sum\n",
    "\n",
    "df = spark.read.csv(\"s3a://data/new_retail_data.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"Total_Purchases\", col(\"Total_Purchases\").cast(\"int\"))\n",
    "df = df.withColumn(\"Age\", col(\"Age\").cast(\"int\"))\n",
    "df = df.withColumn(\"Year\", col(\"Year\").cast(\"string\"))\n",
    "df = df.withColumn(\"Month\", col(\"Month\").cast(\"string\"))\n",
    "df = df.withColumn(\"Transaction_ID\", col(\"Transaction_ID\").cast(\"int\"))\n",
    "df = df.withColumn(\"Customer_ID\", col(\"Customer_ID\").cast(\"int\"))\n",
    "df = df.dropna()\n",
    "df = df.dropDuplicates()\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"BIdatabase.general_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19807580-5b4c-46f5-ac0c-abc53ba56018",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = spark.read.parquet(\"s3a://data/df_rfm\", header=True, inferSchema=True)\n",
    "dfm = dfm.dropna()\n",
    "dfm = dfm.dropDuplicates()\n",
    "dfm.write.mode(\"overwrite\").saveAsTable(\"BIdatabase.rfm_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5333d46-2e83-4adb-a4fa-850d3d75fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|     info_name|          info_value|\n",
      "+--------------+--------------------+\n",
      "|  Catalog Name|       spark_catalog|\n",
      "|Namespace Name|          BIdatabase|\n",
      "|       Comment|                    |\n",
      "|      Location|s3a://data/wareho...|\n",
      "|         Owner|              jovyan|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE database BIdatabase\").show(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
