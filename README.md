# Proyecto de An√°lisis de Datos

![Servicios Usados](imgs/tools.png)

#### **¬øPor qu√© este proyecto?** 
Este proyecto se ha dise√±ado con el objetivo de enfrentar y superar los desaf√≠os t√≠picos que un analista de datos encuentra en su d√≠a a d√≠a. A trav√©s de la aplicaci√≥n de tecnolog√≠as avanzadas como Spark, Hive y Power BI, buscamos no solo demostrar un flujo completo de an√°lisis de datos, sino tambi√©n abordar problemas reales de procesamiento, an√°lisis y visualizaci√≥n de datos.

#### **¬øQu√© estamos analizando y por qu√©?** 
En este proyecto, nos enfocamos en analizar un conjunto de datos de un negocio de retail para extraer informaci√≥n valiosa que pueda ser utilizada en la toma de decisiones estrat√©gicas. Este an√°lisis incluye la limpieza, exploraci√≥n y modelado de datos para entender patrones clave y comportamientos de los usuarios.

#### **¬øQu√© vamos a hacer?**

- Carga de Datos en MinIO
- Lectura de Datos
- An√°lisis Exploratorio de Datos (EDA)
- An√°lisis RFM
- Creaci√≥n de Base de Datos con Hive Metastore
- Visualizaci√≥n en Power BI

![Diagrama del Flujo de Trabajo](imgs/diagram.png)

### Pasos:

#### 1. Carga de Datos en MinIO

El primer paso consiste en cargar la tabla de datos con la que vamos a trabajar en un bucket de MinIO, almacenamiento compatible con S3 que permite manejar datos de manera escalable y segura.

####  2. Lectura de Datos con Pandas y Spark en Jupyter Notebooks

Una vez los datos est√°n cargados en MinIO, se leen utilizando Pandas con Spark integrado desde Jupyter Notebooks. Esta integraci√≥n permite aprovechar las capacidades de procesamiento distribuido de Spark mientras se mantiene la flexibilidad de Pandas para la manipulaci√≥n de datos.

#### 3. An√°lisis Exploratorio de Datos (EDA)

Despu√©s de leer los datos, se realiza un An√°lisis Exploratorio de Datos (EDA) para entender mejor las caracter√≠sticas y la estructura de los datos. Este paso incluye la limpieza de los datos, donde se manejan valores nulos, se corrigen inconsistencias y se preparan los datos para el an√°lisis posterior.

[Notebook: An√°lisis Exploratorio de Datos](/notebooks/EDA_retail.ipynb)

#### 4. An√°lisis RFM

El an√°lisis RFM (Recency, Frequency, Monetary) es una t√©cnica de segmentaci√≥n de clientes basada en su comportamiento de compra. Este an√°lisis ayuda a identificar segmentos clave de clientes seg√∫n:

- **Recency (Recencia):** Cu√°nto tiempo ha pasado desde la √∫ltima compra.
- **Frequency (Frecuencia):** Cu√°ntas veces ha comprado un cliente en un per√≠odo determinado.
- **Monetary (Monetario):** Cu√°nto ha gastado un cliente en total.

Al clasificar a los clientes seg√∫n estos tres par√°metros, es posible identificar a los clientes m√°s valiosos y personalizar estrategias de marketing para cada segmento.

[Notebook: RFM-Analisis](/notebooks/RFM_retail.ipynb)

#### 5. Creaci√≥n de Base de Datos con Hive Metastore

Una vez realizado el an√°lisis RFM y el EDA, los datos se almacenan en una base de datos utilizando Hive Metastore. Usando PySparkSQL, se crea la estructura de la base de datos que luego ser√° utilizada para la visualizaci√≥n de datos en el dashboard.

[Notebook: Creacion de Database](/notebooks/Database.ipynb)

#### 6. Visualizando los Datos con Power BI

Finalmente, los datos almacenados en Hive Metastore se extraen en Power BI para la creaci√≥n de un dashboard interactivo. Este dashboard permite a los usuarios explorar los datos de manera visual y obtener insights valiosos para la toma de decisiones.

![Dashboard - Pagina Macro ](/imgs/macro.png)
![Dashboard - Pagina Comportamientos ](/imgs/comportamiento.png)

#### DevOps Engine - Docker

Finalmente, quiero destacar que todo el proyecto fue desarrollado utilizando Docker. El c√≥digo incluye un Makefile que proporciona los comandos necesarios para ejecutar toda la aplicaci√≥n en cualquier m√°quina que tenga Docker instalado.

Dentro del entorno Docker, tambi√©n est√°n disponibles los siguientes servicios:

- **Spark-Thrift-Server**: Utilizado para exponer nuestras base de datos.
- **Hive**: Utilizado como servicio de metastore.
- **MySQL**: Como almacenamiento para Hive.

Adem√°s de los servicios ya mencionados, como **Jupyter**, **Spark** y **MinIO**.

## Conclusi√≥n

Este proyecto demuestra c√≥mo un flujo de trabajo bien definido y el uso de herramientas modernas pueden facilitar un an√°lisis de datos efectivo y escalable. Desde la carga de datos hasta la visualizaci√≥n en un dashboard, cada paso es crucial para obtener insights que pueden ser utilizados en la toma de decisiones estrat√©gicas.

Eso es todo por ahora. Si te gust√≥, ¬°me encantar√≠a que dejaras una estrella en el repositorio de GitHub! üòâ

